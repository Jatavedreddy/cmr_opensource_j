{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# pipeline\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# nn\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#load and save\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on jupyter noteeboks, it's hard to import modules from other notebooks. For the simplicity, we'll just copy the functions developed in our 'etl' noteebok to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "class FeatureCreation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # print('FeatureCreation initialized')\n",
    "        return None\n",
    "    \n",
    "    # For the fit method, we will pass the parameter x. This is our independent variables. \n",
    "    # This fit method will be called when we fit the pipeline.\n",
    "    def fit(self, x, y=None):\n",
    "        # print('Fit FeatureCreation called')\n",
    "        return self\n",
    "    \n",
    "    # Here, we will perform all of our transformations. For creating features automatically, we could create parameters in the class and pass the column names to them.\n",
    "    # But in this case, since it's for this dataset specific, we will perform transformations in the column names directly into the fit method.\n",
    "    # The transform method is called when we caled fit and also predict in the Pipeline. And that makes sense, since we need to create our features when we will train and when we will predict.\n",
    "    def transform(self, x, y=None):\n",
    "        # print('Transform FeatureCreation called')\n",
    "        # creating a copy to avoid changes to the original dataset\n",
    "        x_ = x.copy()\n",
    "        # print(f'Before Transformation: {x_.shape}')\n",
    "        # and now, we create everyone of our features.\n",
    "        # Area power of two\n",
    "        x_['area2'] = x_['area'] ** 2\n",
    "        # The ratio between area and rooms\n",
    "        x_['area/room'] = x_['area'] / x_['rooms']\n",
    "        # The ratio between area and bathroom\n",
    "        x_['area/bathroom'] = x_['area'] / x_['bathroom']\n",
    "        # the sum of rooms and bathrooms\n",
    "        x_['rooms+bathroom'] = x_['rooms'] + x_['bathroom']\n",
    "        # the product between rooms and bathrooms\n",
    "        x_['rooms*bathroom'] = x_['rooms'] * x_['bathroom']\n",
    "        # the ratio between rooms and bathrooms\n",
    "        x_['rooms/bathroom'] = x_['rooms'] / x_['bathroom']\n",
    "        # the product between hoa and property tax\n",
    "        x_['hoa*property tax'] = x_['hoa (R$)'] * x_['property tax (R$)']\n",
    "        # print(f'After Transformation: {x_.shape}')\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('houses_to_rent_v2_fteng.csv')\n",
    "df = pd.read_csv(os.path.join(os.path.abspath('../data'), 'houses_to_rent_v2_fteng.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>rooms</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>parking spaces</th>\n",
       "      <th>animal</th>\n",
       "      <th>furniture</th>\n",
       "      <th>hoa (R$)</th>\n",
       "      <th>rent amount (R$)</th>\n",
       "      <th>property tax (R$)</th>\n",
       "      <th>fire insurance (R$)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>not acept</td>\n",
       "      <td>furnished</td>\n",
       "      <td>470</td>\n",
       "      <td>2690</td>\n",
       "      <td>172</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>352</td>\n",
       "      <td>1500</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>0</td>\n",
       "      <td>11000</td>\n",
       "      <td>425</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>not acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>850</td>\n",
       "      <td>2550</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>500</td>\n",
       "      <td>1631</td>\n",
       "      <td>192</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             city  area  rooms  bathroom  parking spaces     animal  \\\n",
       "0  Belo Horizonte    42      1         1               1  not acept   \n",
       "1  Belo Horizonte    64      2         2               1      acept   \n",
       "2  Belo Horizonte    80      3         2               1      acept   \n",
       "3  Belo Horizonte   200      4         2               1  not acept   \n",
       "4  Belo Horizonte    45      1         1               1      acept   \n",
       "\n",
       "       furniture  hoa (R$)  rent amount (R$)  property tax (R$)  \\\n",
       "0      furnished       470              2690                172   \n",
       "1  not furnished       352              1500                 80   \n",
       "2  not furnished         0             11000                425   \n",
       "3  not furnished       850              2550                  9   \n",
       "4  not furnished       500              1631                192   \n",
       "\n",
       "   fire insurance (R$)  \n",
       "0                   36  \n",
       "1                   20  \n",
       "2                  181  \n",
       "3                   34  \n",
       "4                   12  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8995, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to padronize the model definition, we will create a Pipeline. That will assure that all the data will pass to same process of transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=['rent amount (R$)'], axis=1)\n",
    "y = df['rent amount (R$)']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's categorize our variables into numerical or categorical. This is required since the transformation of each type it's different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the pipelines for every type of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical\n",
    "catTransformer = Pipeline(steps=[\n",
    "    # For categorical variables, we will use onehotencoder.\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Numerical\n",
    "numTransformer = Pipeline(steps=[\n",
    "    # For numerical features we will use standardscaler because we have already treated the dataset for outliers.\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's integrate those pipelines with a ColumnTransformer and create our preprocessor. Everytime that we wan't to predict, this preprocessor will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numTransformer, selector(dtype_exclude=[\"category\", \"object\"])),\n",
    "        ('categoric', catTransformer, selector(dtype_include=[\"category\", \"object\"]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable it's rent amount (R$), that means that we need a supervised machine learning model and our target it is a continous variable, a regression problem.\n",
    "\n",
    "I've chose to use RMSE as our metric because it is more sensible to outliers than MAE, so it give us a more wide comprehension if that is affecting our model.\n",
    "\n",
    "For validation, we will use K-Fold Cross Validation. That means that the data will be divided by K groups of samples, called folds. Then, in every iteration of K, the data will be trained in K-1 and tested in the rest. Below we have a example of how K-Fold works, according to the sci-kit learn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then define our cross validation function with our validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 5 folds\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True).get_n_splits(df.values)\n",
    "    # here we define that our scoring metric will be rmse for every iteration of the cross validation\n",
    "    rmse = np.sqrt(-cross_val_score(model, x_train, y_train,\n",
    "                   scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to define a few base models to validate our metric.\n",
    "\n",
    "We will also do a Random Search to tune our models. We chose to use random search instead of grid search because [researchs](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) shows that random search do a better job.\n",
    "\n",
    "For the baseline we will use these models:\n",
    "- RandomForest\n",
    "- XGBoost\n",
    "- LGBM\n",
    "- Ridge\n",
    "- Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the topology of the neural network.\n",
    "\n",
    "The number of hidden layers and neurons it's a import factor to make the model generalize well.\n",
    "\n",
    "From Introduction to Neural Networks for Java (second edition) by Jeff Heaton, there are two decisions to be made regarding to the hidden layers:\n",
    "\n",
    "1 - How many hidden layers to actually have in the neural network .\n",
    "\n",
    "> Problems that require two or more hidden layers are rarely encountered.\n",
    "\n",
    "2 - How many neurons will be in each of these layers.\n",
    "\n",
    "> - The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "> - The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "> - The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', dropout=0.2, activation='relu', kernel_initializer='normal'):\n",
    "    # We initialized every parameter that we want to optimize in the neural network.\n",
    "    # Let's initialize our sequential model\n",
    "    model = Sequential()\n",
    "    # Our input layer will have 15 neurons, that's equal to the input_dim\n",
    "    model.add(Dense(units = 15, activation = activation, input_dim = 15, kernel_initializer=kernel_initializer))\n",
    "    # In every step we will create a dropout layer in order to optimize this parameter and prevent overfitting.\n",
    "    model.add(Dropout(dropout))\n",
    "    # Ni = number of input neurons\n",
    "    # No = number of output neurons\n",
    "    # Our nn will have only one hidden layer, and the number of neurons follow the rule: 2/3 * (Ni) + No  = 11.\n",
    "    model.add(Dense(units = 11, activation = activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    # Our output layer have only one neuron, since it's a regression problem.\n",
    "    model.add(Dense(units = 1, activation = activation))\n",
    "\n",
    "    # now we compile our model utilizing the mean squared error loss function\n",
    "    model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a function to apply the Random Search through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "def get_best_param(model, custom_features=False, custom_target=False):\n",
    "    \"\"\"\n",
    "    This function return a random search object.\n",
    "    \"\"\"\n",
    "    # cachedir = mkdtemp()\n",
    "\n",
    "    # The model will be identified by a string and for each one, we will set a parameter grid. This grid will be passed to the random search\n",
    "    # Defined the model and the parameter grid, we instantiate the Pipeline.\n",
    "    # Since we are using a Pipeline, we have to optimize the parameters of our model, and to do that we will have to name the step in the pipeline and access that in the pipeline.\n",
    "    # In our case, our step will be named model. To access this parameters we will add a model__ in front of every parameter.\n",
    "    if model == 'RandomForest':\n",
    "        random_grid = {\n",
    "            'model__n_estimators': [int(x) for x in np.linspace(start = 200, stop = 800, num = 4)],\n",
    "            'model__max_features': ['auto', 'sqrt'],\n",
    "            'model__max_depth': [i for i in np.arange(1, 10)],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4],\n",
    "            'model__bootstrap': [True, False]\n",
    "        }\n",
    "\n",
    "        regressor_model = RandomForestRegressor()\n",
    "\n",
    "    elif model == 'XGB':\n",
    "        random_grid = {\n",
    "            \"model__n_estimators\":[int(x) for x in np.linspace(start = 200, stop = 800, num = 4)],\n",
    "            \"model__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
    "            \"model__max_depth\"        : [i for i in np.arange(1, 10)],\n",
    "            \"model__min_child_weight\" : [1e-3, 1, 3, 5, 7 ],\n",
    "            \"model__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "            \"model__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7, 1 ] \n",
    "        }\n",
    "\n",
    "        regressor_model = xgb.XGBRegressor()\n",
    "\n",
    "    elif model == 'LGBM':\n",
    "        random_grid = {\n",
    "            \"model__n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "            \"model__boosting_type\": ['dart', 'goss'],\n",
    "            \"model__max_depth\": [i for i in np.arange(1, 51)],\n",
    "            \"model__num_leaves\": [int(x) for x in np.linspace(start = 10, stop = 2000, num = 10)],\n",
    "            \"model__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
    "            \"model__min_child_weight\" : [1e-3, 1, 3, 5, 7 ],\n",
    "            \"model__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7, 1 ],\n",
    "        }\n",
    "\n",
    "        regressor_model = lgb.LGBMRegressor()\n",
    "\n",
    "    elif model == 'Ridge':\n",
    "        random_grid = {\n",
    "            \"model__alpha\": np.linspace(start=0.001, stop=1,  num=101),\n",
    "            \"model__fit_intercept\": [True, False]\n",
    "        }\n",
    "\n",
    "        regressor_model = Ridge()\n",
    "\n",
    "    elif model == 'NeuralNetwork':\n",
    "        random_grid = {\n",
    "            \"model__batch_size\": (32, 64, 128, 256),\n",
    "            \"model__epochs\": (50, 100, 200, 300),\n",
    "            \"model__activation\": ('relu', 'tanh', 'linear'),\n",
    "            \"model__dropout\": (0.0, 0.1, 0.2, 0.3),\n",
    "            \"model__kernel_initializer\": ('glorot_uniform','normal','uniform'),\n",
    "            \"model__optimizer\": ('SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl')\n",
    "        }\n",
    "\n",
    "    \n",
    "        # Since we are passing our model to a sci-kit learn Pipeline, we need to wrap our Keras model first.\n",
    "        regressor_model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "    # For the neural network, we have a particularity. We have to pass to the first layer the input dimension.\n",
    "    # Since we are doing transformations in our dataset, such as One Hot encoding, we don't know for sure how many features will exist.\n",
    "    # One way around that it is define a fix value of variables that will be passed, and this is possible by using SelecKBest from sci-kit learn.\n",
    "    # This function scores the variables according to the function passed, in our case, f_regression, and return the k variables defined.\n",
    "    # Defined the number of features, we just pass the input dim in our create_model function above and create another step in our Pipeline.\n",
    "    \n",
    "\n",
    "    # We will test for every possible combination regarding to the target transformation and the feature engineering and compute the results.\n",
    "    if custom_features:\n",
    "        if model == 'NeuralNetwork':\n",
    "            select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
    "            model_pipeline = Pipeline(steps=[\n",
    "                        ('featurecreation', FeatureCreation()),\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('select_k_best', select_best_features),\n",
    "                        ('model', regressor_model)\n",
    "                    ])\n",
    "        else:\n",
    "            model_pipeline = Pipeline(steps=[\n",
    "                        ('featurecreation', FeatureCreation()),\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('model', regressor_model)\n",
    "                    ])\n",
    "    else:\n",
    "        if model == 'NeuralNetwork':\n",
    "            select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
    "            model_pipeline = Pipeline(steps=[\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('select_k_best', select_best_features),\n",
    "                        ('model', regressor_model)\n",
    "                    ])\n",
    "        else:\n",
    "            model_pipeline = Pipeline(steps=[\n",
    "                        ('preprocessor', preprocessor),\n",
    "                        ('model', regressor_model)\n",
    "                    ])\n",
    "\n",
    "    kf = KFold(n_folds, shuffle=True).get_n_splits(df.values)\n",
    "\n",
    "    if custom_target:\n",
    "        custom_pipeline = TransformedTargetRegressor(\n",
    "            regressor=model_pipeline,\n",
    "            func=np.log,\n",
    "            inverse_func=np.exp\n",
    "            )\n",
    "\n",
    "        for old_key in list(random_grid.keys()):\n",
    "            random_grid['regressor__' + old_key] = random_grid.pop(old_key)\n",
    "        rzsearch = RandomizedSearchCV(estimator=custom_pipeline, param_distributions=random_grid, cv=kf, n_jobs=-1)\n",
    "    else:\n",
    "        rzsearch = RandomizedSearchCV(estimator=model_pipeline, param_distributions=random_grid, cv=kf, n_jobs=-1)\n",
    "\n",
    "    return rzsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's iterate through every model, run the random search in each one and return a dataframe with the informations.\n",
    "def result_cv_models(custom_features=False, custom_target=False):\n",
    "    today = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    models = ['RandomForest', 'XGB', 'LGBM', 'Ridge', 'NeuralNetwork']\n",
    "    best_models = dict()\n",
    "    results_dict = dict()\n",
    "    \n",
    "    if custom_target:\n",
    "        for model in models:\n",
    "            rzsearch = get_best_param(model, custom_features=custom_features, custom_target=custom_target).fit(x_train, y_train)\n",
    "            best_models[model] = (rzsearch.best_estimator_.regressor_['model'], [rzsearch.best_params_])\n",
    "    else:\n",
    "        for model in models:\n",
    "            rzsearch = get_best_param(model, custom_features=custom_features, custom_target=custom_target).fit(x_train, y_train)\n",
    "            best_models[model] = (rzsearch.best_estimator_['model'], [rzsearch.best_params_])\n",
    "\n",
    "    for name, model in best_models.items():\n",
    "\n",
    "        if custom_features:\n",
    "            if name == 'NeuralNetwork':\n",
    "                select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
    "                model_pipeline = Pipeline(steps=[\n",
    "                            ('featurecreation', FeatureCreation()),\n",
    "                            ('preprocessor', preprocessor),\n",
    "                            ('select_k_best', select_best_features),\n",
    "                            ('model', model[0])\n",
    "                        ])\n",
    "            else:\n",
    "                model_pipeline = Pipeline(steps=[\n",
    "                            ('featurecreation', FeatureCreation()),\n",
    "                            ('preprocessor', preprocessor),\n",
    "                            ('model', model[0])\n",
    "                        ])\n",
    "        else:\n",
    "            if name == 'NeuralNetwork':\n",
    "                select_best_features = SelectKBest(k=15, score_func=f_regression)\n",
    "                model_pipeline = Pipeline(steps=[\n",
    "                            ('preprocessor', preprocessor),\n",
    "                            ('select_k_best', select_best_features),\n",
    "                            ('model', model[0])\n",
    "                        ])\n",
    "            else:\n",
    "                model_pipeline = Pipeline(steps=[\n",
    "                            ('preprocessor', preprocessor),\n",
    "                            ('model', model[0])\n",
    "                        ])\n",
    "\n",
    "        # If we are performing a target transformation, we have to pass the pipeline to our TransformedTargetRegressor object.\n",
    "        if custom_target:\n",
    "            custom_pipeline = TransformedTargetRegressor(\n",
    "                regressor=model_pipeline,\n",
    "                func=np.log,\n",
    "                inverse_func=np.exp\n",
    "                )\n",
    "\n",
    "            scores = rmsle_cv(custom_pipeline)\n",
    "\n",
    "            # save the model\n",
    "            custom_pipeline.fit(x_train, y_train)\n",
    "\n",
    "            predict_test = custom_pipeline.predict(x_test)\n",
    "\n",
    "            rmse_testset = mean_squared_error(y_test, predict_test, squared=False)\n",
    "            mae_testset = mean_absolute_error(y_test, predict_test)\n",
    "\n",
    "            # for the neural network we need additional steps.\n",
    "            if name == 'NeuralNetwork':\n",
    "                # The keras model it is not serialized by pickle. To get around that, we save the model using the keras save method.\n",
    "                # Similar to the pipeline object, to access the model, we have to dig into the steps of the transformed object, and then dig into the pipeline object.\n",
    "                # custom_pipeline.regressor_.named_steps['model'].model.save(os.path.join(os.path.abspath(''), \"models\", f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
    "                custom_pipeline.regressor_.named_steps['model'].model.save(os.path.join(os.path.abspath('../models'), f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
    "                # Then, we set the model inside the pipeline equals to None to be able to serialize.\n",
    "                custom_pipeline.regressor_.named_steps['model'].model = None\n",
    "\n",
    "                # Into the transformed object we have to set the regressor equals None also.\n",
    "                # Later, we will load the pipeline and the model and add the model to the pipeline again.\n",
    "                custom_pipeline.regressor.set_params(model = None)\n",
    "\n",
    "            # Now, serialize and save the model.\n",
    "            # with open(os.path.join(os.path.abspath(''), \"models\", f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
    "            with open(os.path.join(os.path.abspath('../models'), f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
    "                pickle.dump(custom_pipeline, f, -1)\n",
    "\n",
    "        else:\n",
    "            scores = rmsle_cv(model_pipeline)\n",
    "\n",
    "            #save the model\n",
    "            model_pipeline.fit(x_train, y_train)\n",
    "\n",
    "            predict_test = model_pipeline.predict(x_test)\n",
    "\n",
    "            rmse_testset = mean_squared_error(y_test, predict_test, squared=False)\n",
    "            mae_testset = mean_absolute_error(y_test, predict_test)\n",
    "\n",
    "            if name == 'NeuralNetwork':\n",
    "                # model_pipeline.named_steps['model'].model.save(os.path.join(os.path.abspath(''), \"models\", f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
    "                model_pipeline.named_steps['model'].model.save(os.path.join(os.path.abspath('../models'), f'v1_model_{name}_{round(scores.mean(), 3)}_{today}.h5'))\n",
    "                model_pipeline.named_steps['model'].model = None\n",
    "\n",
    "            # with open(os.path.join(os.path.abspath(''), \"models\", f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
    "            with open(os.path.join(os.path.abspath('../models'), f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"), 'wb') as f:\n",
    "                pickle.dump(model_pipeline, f, -1)\n",
    "\n",
    "        # Here we will save our results. One important column it is the 'pipe_file_name', this will be used to load our model later.\n",
    "        results_dict[name] = {'name': name, 'model': model[0], 'params': model[1], 'rmse_cv': round(np.mean(scores), 3), 'std_cv': round(np.std(scores), 3), 'rmse_testset': rmse_testset, 'mae_testset': mae_testset, 'custom_features': custom_features, 'custom_target': custom_target, 'all_scores_cv': scores, 'pipe_file_name': f\"v1_pipe_{name}_{round(scores.mean(), 3)}_{today}.pickle\"}\n",
    "\n",
    "    results_df = pd.DataFrame(results_dict).T\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_feature': True, 'custom_target': True}\n",
      "{'custom_feature': True, 'custom_target': False}\n",
      "{'custom_feature': False, 'custom_target': True}\n",
      "{'custom_feature': False, 'custom_target': False}\n"
     ]
    }
   ],
   "source": [
    "all_results_df = list()\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "combinations = ({'custom_feature': True, 'custom_target': True}, {'custom_feature': True, 'custom_target': False}, {'custom_feature': False, 'custom_target': True}, {'custom_feature': False, 'custom_target': False})\n",
    "\n",
    "# We have 4 possible combinations, let's get the results of each one of them.\n",
    "for combination in combinations:\n",
    "    print(combination)\n",
    "    results_df = result_cv_models(custom_features=combination['custom_feature'], custom_target=combination['custom_target'])\n",
    "    all_results_df.append(results_df)\n",
    "\n",
    "end = time.time()\n",
    "time_run = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2496.265648126602"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(i for i in all_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(os.path.join(os.path.abspath('../data'), 'model_evaluation.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc33febecb25bf51ecb9e8745ca02a2d49ab50b5c3e3d57ffb721b0f2206d5d8"
  },
  "kernelspec": {
   "display_name": "mll",
   "language": "python",
   "name": "mll"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
